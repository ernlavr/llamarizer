# control
wandb_mode : "offline"
save_model_at_end : False
train_summ : True         # train summarizer
train_with_nli : False    # train summarizer with an NLI module
train_nli : False         # for training the NLI
eval_summ : False         # for entering the main summarizer evaluation script

# Hyperparameters
learning_rate: 1e-3
epochs: 2
weight_decay : 0.01
batch_size : 16
eval_batch_size : 16
sequence_length : 512
model_name : "meta-llama/Llama-2-7b-hf"
repetition_penalty : 1.25
eval_steps : 100  # processing steps before evaluation takes place

# evaluation
wandb_num_examples : 10  # number of examples to save in W&B table

# dataset
train_size: 50000
val_size: 5000
use_prompt: True

# Quantization (necessary for Llama2)
load_in_4bit: True
