""" Developed by Viktor Due Pedersen

This file contains the code for the Natural Language Inference (NLI) model.

Finetuned on the X-Sum Factuality dataset from huggingface:
    https://huggingface.co/datasets/xsum_factuality

The model is based on AutoModelForSequenceClassification from huggingface:
    https://huggingface.co/docs/transformers/model_doc/auto

Developed on DistilBert, but can be changed to any model from huggingface:
    https://huggingface.co/docs/transformers/model_doc/distilbert

"""
import os
import wandb
import pandas as pd
from dataclasses import dataclass, field
from typing import Dict, List, Set

from datasets import Dataset, load_dataset
from datasets.dataset_dict import DatasetDict
#from dotenv import load_dotenv
from sklearn.metrics import accuracy_score, precision_recall_fscore_support
import sklearn.utils as skUtils
from transformers import (
    AutoConfig,
    AutoModelForSequenceClassification,
    AutoTokenizer,
    Trainer,
    TrainingArguments,
)
from transformers.tokenization_utils_base import BatchEncoding
from transformers.trainer_utils import EvalPrediction
import src.utils.NliTrainer as nt
import numpy as np

# .env file contains the WANDB_API_KEY and WANDB_PROJECT
#load_dotenv()

@dataclass
class NLI_Finetune:

    def __init__(self):
        # Hyper parameters
        self.HF_MODEL_NAME: str = wandb.config.model_name
        self.learning_rate: float = wandb.config.learning_rate
        self.weight_decay: float = wandb.config.weight_decay
        self.epochs: int = wandb.config.epochs
        self.train_batch_size: int = wandb.config.batch_size
        self.eval_steps = wandb.config.eval_steps
        
        # Tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(self.HF_MODEL_NAME)
        self.model = AutoModelForSequenceClassification.from_pretrained(
            self.HF_MODEL_NAME, num_labels=2
        )
        
        # Datasets
        self.dataset_factuality: DatasetDict = load_dataset("xsum_factuality")
        self.dataset_xsum: DatasetDict = load_dataset("EdinburghNLP/xsum", split="train+validation+test")
        
        self.merged_dataset: Dataset = None
        self.dataset: DatasetDict = None
        self.post_init()
 
    def post_init(self):
        self.merge_datasets()

        # The is_factual column has NULL value for some entries which have been replaced iwth -1. These are removed.
        self.merged_dataset = self.merged_dataset.filter(
            lambda example: example["is_factual"] != -1
        )

        # split into train and validation
        self.split_dataset()

    def merge_datasets(self):
            """Should merge the X-sum dataset with the X-sum factuality dataset based on the bbcid. The two datasets have
            different columns, so the merge should be done on the bbcid column.

            dataset_factuality:
                This dataset consists of annotated summaries generated by 4 systems
                BERTS2S (len: 1376)
                PtGen (len: 1375)
                TConvS2S (len: 1422)
                TranS2S (len: 1424)

                bbcid: Document id in the XSum corpus.
                system: Name of neural summarizer.
                summary: Summary generated by â€˜systemâ€™.
                is_factual: Yes (1) or No (0)
                worker_id: Worker ID (one of 'wid_0', 'wid_1', 'wid_2')

            dataset_xsum:
                document: Input news article.
                summary: One sentence summary of the article.
                id: BBC ID of the article.
            """

            # rename the bbcid column to id
            self.dataset_factuality = self.dataset_factuality.rename_column(
                "bbcid", "id"
            )

            """
            dataset_factuality:
                DatasetDict({
                    train: Dataset({
                        features: ['id', 'system', 'summary', 'is_factual', 'worker_id'],
                        num_rows: 5597 }) })

            dataset_xsum:
                Dataset({
                    features: ['document', 'summary', 'id'],
                    num_rows: 226711 })
            """

            # Convert the xsum dataset to a dictionary for quick lookup
            xsum_dict: Dict[int, Dict] = {
                int(example["id"]): example for example in self.dataset_xsum
            }

            # Prepare a new dataset
            merged_data = {
                "id": [],
                "system": [],
                "document": [],
                "summary": [],
                "is_factual": [],
            }

            # Create a pandas dataframe from the factuality dataset
            self.fact_df = pd.DataFrame(self.dataset_factuality['train'])

            # First we loop over IDs, 
            for examples in self.fact_df.groupby("id"):
                # Then for ID, we group by system (BERTS2S, PtGen, TConvS2S, TranS2S)
                for system in examples[1].groupby("system"):
                    # Extract the ID and then reference just the group's dataframe
                    factuality_id: int = examples[0]
                    system_name = system[0]
                    system_examples = system[1]
                    xsum_example = xsum_dict.get(factuality_id)
                    is_factual = int(system_examples["is_factual"].median())

                    if is_factual != -1 and xsum_example:
                        id = factuality_id
                        document = xsum_example['document'] # document from original
                        summary = system_examples["summary"].values[0] # summary from XSum-Fact
                        is_factual = int(is_factual)

                        merged_data["id"].append(id)
                        merged_data["system"].append(system_name)
                        merged_data["document"].append(document)
                        merged_data["summary"].append(summary)
                        merged_data["is_factual"].append(is_factual)

            self.merged_dataset = Dataset.from_dict(merged_data)

            """
            merged_dataset:
                Dataset({
                    features: ['id', 'document', 'summary', 'is_factual'],
                    num_rows: 5597 })
            """

    def split_dataset(self):
        """Since the X-sum dataset only contains a train set, we split it into train and validation set.
        Note that the texts are replicated three times and should be grouped by "bbcid
        """
        bbcids: Set[str] = set(self.merged_dataset["id"])

        # split the bbcids into train and validation
        train_bbcids = set(list(bbcids)[: int(len(bbcids) * 0.8)])
        val_bbcids = bbcids - train_bbcids

        # filter the dataset

        self.dataset = DatasetDict()

        self.dataset["val"] = self.merged_dataset.filter(
            lambda example: example["id"] in val_bbcids
        )
        self.dataset["train"] = self.merged_dataset.filter(
            lambda example: example["id"] in train_bbcids
        )

        if wandb.config.upsample_train is not None:
            train_upsampled = self.resample(self.dataset["train"].to_pandas(), upsample=False)
            self.dataset['train'] = train_upsampled

        if wandb.config.upsample_val is not None:
            val_downsampled = self.resample(self.dataset["val"].to_pandas(), upsample=True)
            self.dataset['val'] = val_downsampled
            
            
        # make sure that there don't exist a bbcid in both train and val
        train_bbcids = set(self.dataset["train"]["id"])
        val_bbcids = set(self.dataset["val"]["id"])
        assert (
            len(train_bbcids.intersection(val_bbcids)) == 0
        ), "The same id exists in both train and val"

    def resample(self, data_split, upsample=True) -> Dataset:
        # Fetch factual and non-factual subsplits
        non_factual = data_split[data_split["is_factual"] == 0]
        factual = data_split[data_split["is_factual"] == 1]
        minority = non_factual if len(non_factual) < len(factual) else factual
        majority = factual if len(factual) > len(non_factual) else non_factual

        if upsample:
            # Upsample the minority class to match the majority class
            factual_upsampled = skUtils.resample(minority, replace=True, n_samples=len(majority), random_state=42)
            concatenated = [factual_upsampled, majority]
        else:
            # Downsample the majority class to match the minority class
            downsampled_majority = skUtils.resample(majority, replace=True, n_samples=len(minority), random_state=42)
            concatenated = [minority, downsampled_majority]

        # Prepare and return output
        upsampled_data_split = pd.concat(concatenated)
        output = Dataset.from_pandas(upsampled_data_split).shuffle(seed=42)
        return output

    def compute_metrics(self, pred: EvalPrediction):
        labels = pred.label_ids
        preds = pred.predictions.argmax(-1)
        precision, recall, f1, _ = precision_recall_fscore_support(
            labels, preds, average="weighted", zero_division=0
        )
        acc = accuracy_score(labels, preds)
        return {"accuracy": acc, "f1": f1, "precision": precision, "recall": recall}

    def save_model(self, trainer):
        # save the checkpoints of best performing model
        run_id = wandb.run.id
        trainer.save_model(f"./results/best_model_{run_id}")

        # upload the best model weights to wandb
        artifact = wandb.Artifact(f'model_{run_id}', type='model')
        artifact.add_dir(f"./results/best_model_{run_id}")
        wandb.run.log_artifact(artifact)
        artifact.wait() # wait for artifact to finish uploading

    def finetune(self):
        def tokenize_function(examples: Dict) -> BatchEncoding:
            # `id: Document id in the XSum corpus.
            id: List[int] = examples["id"]
            # document:
            document: List[str] = examples["document"]
            # summary: Summary generated by â€˜systemâ€™.
            summary: List[str] = examples["summary"]
            # is_factual: Yes (1) or No (0)
            is_factual: List[int] = examples["is_factual"]  # 0, 1

            tokenized_inputs = self.tokenizer(
                text=document,  # First part of the input
                text_pair=summary,  # Second part of the input
                truncation="only_first",  # Truncate only the first part (document) if the combined input is too long
                padding="max_length",
                max_length=512,  # Or any other max length that suits your model
            )

            tokenized_inputs["labels"] = is_factual

            return tokenized_inputs

        tokenized_datasets: DatasetDict = self.dataset.map(
            tokenize_function, batched=True
        )

        os.environ["WANDB_LOG_MODEL"] = "checkpoint"
        os.environ["TOKENIZERS_PARALLELISM"] = "false"

        # Ensure that the WANDB_API_KEY and WANDB_PROJECT are not None or empty
        wandb_api_key = os.getenv("WANDB_API_KEY")
        wandb_project = os.getenv("WANDB_PROJECT")
        if not wandb_api_key or not wandb_project:
            raise ValueError(
                "WANDB_API_KEY and WANDB_PROJECT must be set in the environment"
            )
        
        # Calculate the warmup steps -> 0.5 of steps in an epoch
        self.warmup_steps = int(len(tokenized_datasets["train"]) * self.epochs / self.train_batch_size / 2)
        # calculate class_weights based on train ["is_factual"] label
        class_weights = skUtils.class_weight.compute_class_weight(
            "balanced", classes=[0, 1], y=tokenized_datasets["train"]["is_factual"]
        )

        # Training the model with WANDB parameters
        training_args = TrainingArguments(
            # Logging
            report_to="wandb",
            run_name="nli_finetuning_run",  # Optionally, add a run name
            output_dir="./results",
            logging_dir="./logs",
            logging_steps=10,

            # Eval
            do_eval=True,
            evaluation_strategy="steps",
            save_strategy="steps",
            save_steps=1000,
            eval_steps=self.eval_steps,

            # Hyperparameters
            learning_rate=self.learning_rate,
            warmup_ratio=0.1,
            weight_decay=self.weight_decay,
            num_train_epochs=self.epochs,
            per_device_train_batch_size=self.train_batch_size,
            per_device_eval_batch_size=self.train_batch_size,
            metric_for_best_model="eval_loss",
            optim="adamw_torch",

            # Huggingface
            load_best_model_at_end=True,
            hub_token="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",
            push_to_hub_model_id=self.HF_MODEL_NAME + "-xsum-factuality",
        )

        trainer = nt.NliTrainer(
            class_weights=class_weights,
            model=self.model,  # the instantiated ðŸ¤— Transformers model to be trained
            args=training_args,  # training arguments, defined above
            train_dataset=tokenized_datasets["train"],  # training dataset
            eval_dataset=tokenized_datasets["val"],  # evaluation dataset
            compute_metrics=self.compute_metrics,
        )

        trainer.train()

        if wandb.config.save_model_at_end:
            self.save_model(trainer)


if __name__ == "__main__":
    nli = NLI_Finetune()
    nli.finetune()
