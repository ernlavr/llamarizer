""" Developed by Viktor Due Pedersen

This file contains the code for the Natural Language Inference (NLI) model.

Finetuned on the X-Sum Factuality dataset from huggingface:
    https://huggingface.co/datasets/xsum_factuality

The model is based on AutoModelForSequenceClassification from huggingface:
    https://huggingface.co/docs/transformers/model_doc/auto

Developed on DistilBert, but can be changed to any model from huggingface:
    https://huggingface.co/docs/transformers/model_doc/distilbert

"""
import os
from dataclasses import dataclass, field
from typing import Dict, List

from datasets import load_dataset
from datasets.dataset_dict import DatasetDict
from dotenv import load_dotenv
from transformers import (
    AutoConfig,
    AutoModelForSequenceClassification,
    AutoTokenizer,
    Trainer,
    TrainingArguments,
)
from transformers.tokenization_utils_base import BatchEncoding

load_dotenv()  # contains "WANDB_API_KEY" and "WANDB_PROJECT"


@dataclass
class NLI_Finetune:
    HF_MODEL_NAME: str = "distilbert-base-uncased"
    tokenizer = AutoTokenizer.from_pretrained(HF_MODEL_NAME)
    dataset: DatasetDict = field(
        default_factory=lambda: load_dataset("xsum_factuality")
    )

    def __post_init__(self):
        self.config = AutoConfig.from_pretrained(self.HF_MODEL_NAME, num_labels=3)
        self.model = AutoModelForSequenceClassification.from_pretrained(
            self.HF_MODEL_NAME, config=self.config
        )

    def split_dataset(self):
        """Since the X-sum dataset only contains a train set, we split it into train and validation set."""
        self.dataset = self.dataset["train"].train_test_split(test_size=0.1, seed=42)

    def finetune(self):
        def tokenize_function(examples: Dict) -> BatchEncoding:
            # `bbcid1: Document id in the XSum corpus.
            bbcid: List[int] = examples["bbcid"]
            # system: Name of neural summarizer.
            system: List[str] = examples["system"]
            # summary: Summary generated by â€˜systemâ€™.
            summary: List[str] = examples["summary"]
            # is_factual: Yes (1) or No (0)
            is_factual: List[int] = examples["is_factual"]  # -1, 0, 1
            # worker_id: Worker ID (one of 'wid_0', 'wid_1', 'wid_2')
            worker_id: List[str] = examples["worker_id"]

            # Tokenize all texts and align the labels with them.
            tokenized_inputs = self.tokenizer(
                summary,
                truncation=True,
                padding="max_length",
                max_length=128,
            )

            tokenized_inputs["labels"] = is_factual

            return tokenized_inputs

        self.split_dataset()

        tokenized_datasets: DatasetDict = self.dataset.map(
            tokenize_function, batched=True
        )

        # Fetch the WANDB_API_KEY and WANDB_PROJECT from the environment
        wandb_api_key = os.getenv("WANDB_API_KEY")
        wandb_project = os.getenv("WANDB_PROJECT")
        os.environ["WANDB_LOG_MODEL"] = "checkpoint"
        os.environ["TOKENIZERS_PARALLELISM"] = "false"

        # Ensure that the WANDB_API_KEY and WANDB_PROJECT are not None or empty
        if not wandb_api_key or not wandb_project:
            raise ValueError(
                "WANDB_API_KEY and WANDB_PROJECT must be set in the environment"
            )

        # Training the model with WANDB parameters
        training_args = TrainingArguments(
            report_to="wandb",
            run_name="nli_finetuning_run",  # Optionally, add a run name
            output_dir="./results",
            logging_dir="./logs",
            logging_steps=10,
            do_eval=True,
            evaluation_strategy="steps",
            save_strategy="steps",
            save_steps=1000,
            eval_steps=25,
        )

        trainer = Trainer(
            model=self.model,  # the instantiated ðŸ¤— Transformers model to be trained
            args=training_args,  # training arguments, defined above
            train_dataset=tokenized_datasets["train"],  # training dataset
            eval_dataset=tokenized_datasets["test"],  # evaluation dataset
        )

        trainer.train()

        # Evaluate the model
        trainer.evaluate()

        # Save the model
        trainer.save_model("./models/nli_finetuned_model")


if __name__ == "__main__":
    nli = NLI_Finetune()
    nli.finetune()
